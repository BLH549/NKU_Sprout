Populating train/val lists...
Batches per Epoch: 765 | Batch Size: 32
Validation batches per Epoch: 86 | Batch Size: 32
EPOCH 0000 | Batch 100/765 | Loss: 0.164232
EPOCH 0000 | Batch 200/765 | Loss: 0.152307
EPOCH 0000 | Batch 300/765 | Loss: 0.014623
EPOCH 0000 | Batch 400/765 | Loss: 0.016852
EPOCH 0000 | Batch 500/765 | Loss: 0.015386
EPOCH 0000 | Batch 600/765 | Loss: 0.013390
EPOCH 0000 | Batch 700/765 | Loss: 0.012932
===> EPOCH: 0000 | Train Loss: 0.0631 | Val PSNR: 18.37 | LR: 0.001000
[*] Best Model Updated! PSNR: 18.37
EPOCH 0001 | Batch 100/765 | Loss: 0.015331
EPOCH 0001 | Batch 200/765 | Loss: 0.019379
EPOCH 0001 | Batch 300/765 | Loss: 0.013397
EPOCH 0001 | Batch 400/765 | Loss: 0.012983
EPOCH 0001 | Batch 500/765 | Loss: 0.015672
EPOCH 0001 | Batch 600/765 | Loss: 0.011521
EPOCH 0001 | Batch 700/765 | Loss: 0.013502
===> EPOCH: 0001 | Train Loss: 0.0154 | Val PSNR: 18.44 | LR: 0.001000
[*] Best Model Updated! PSNR: 18.44
EPOCH 0002 | Batch 100/765 | Loss: 0.016455
EPOCH 0002 | Batch 200/765 | Loss: 0.015530
EPOCH 0002 | Batch 300/765 | Loss: 0.015811
EPOCH 0002 | Batch 400/765 | Loss: 0.016470
EPOCH 0002 | Batch 500/765 | Loss: 0.015734
EPOCH 0002 | Batch 600/765 | Loss: 0.013087
EPOCH 0002 | Batch 700/765 | Loss: 0.015487
===> EPOCH: 0002 | Train Loss: 0.0152 | Val PSNR: 18.36 | LR: 0.001000
EPOCH 0003 | Batch 100/765 | Loss: 0.011063
EPOCH 0003 | Batch 200/765 | Loss: 0.017109
EPOCH 0003 | Batch 300/765 | Loss: 0.010308
EPOCH 0003 | Batch 400/765 | Loss: 0.011943
EPOCH 0003 | Batch 500/765 | Loss: 0.014073
EPOCH 0003 | Batch 600/765 | Loss: 0.013429
EPOCH 0003 | Batch 700/765 | Loss: 0.011948
===> EPOCH: 0003 | Train Loss: 0.0150 | Val PSNR: 18.45 | LR: 0.001000
[*] Best Model Updated! PSNR: 18.45
EPOCH 0004 | Batch 100/765 | Loss: 0.014662
EPOCH 0004 | Batch 200/765 | Loss: 0.016472
EPOCH 0004 | Batch 300/765 | Loss: 0.015149
EPOCH 0004 | Batch 400/765 | Loss: 0.017521
EPOCH 0004 | Batch 500/765 | Loss: 0.015247
EPOCH 0004 | Batch 600/765 | Loss: 0.014502
EPOCH 0004 | Batch 700/765 | Loss: 0.015556
===> EPOCH: 0004 | Train Loss: 0.0150 | Val PSNR: 18.53 | LR: 0.001000
[*] Best Model Updated! PSNR: 18.53
EPOCH 0005 | Batch 100/765 | Loss: 0.014862
EPOCH 0005 | Batch 200/765 | Loss: 0.014592
EPOCH 0005 | Batch 300/765 | Loss: 0.012309
EPOCH 0005 | Batch 400/765 | Loss: 0.018486
EPOCH 0005 | Batch 500/765 | Loss: 0.020224
EPOCH 0005 | Batch 600/765 | Loss: 0.014042
EPOCH 0005 | Batch 700/765 | Loss: 0.013170
===> EPOCH: 0005 | Train Loss: 0.0149 | Val PSNR: 18.39 | LR: 0.001000
EPOCH 0006 | Batch 100/765 | Loss: 0.015372
EPOCH 0006 | Batch 200/765 | Loss: 0.013334
EPOCH 0006 | Batch 300/765 | Loss: 0.013972
EPOCH 0006 | Batch 400/765 | Loss: 0.013490
EPOCH 0006 | Batch 500/765 | Loss: 0.016540
EPOCH 0006 | Batch 600/765 | Loss: 0.013484
EPOCH 0006 | Batch 700/765 | Loss: 0.018877
===> EPOCH: 0006 | Train Loss: 0.0149 | Val PSNR: 18.56 | LR: 0.001000
[*] Best Model Updated! PSNR: 18.56
EPOCH 0007 | Batch 100/765 | Loss: 0.017580
EPOCH 0007 | Batch 200/765 | Loss: 0.013582
EPOCH 0007 | Batch 300/765 | Loss: 0.014400
EPOCH 0007 | Batch 400/765 | Loss: 0.016734
EPOCH 0007 | Batch 500/765 | Loss: 0.017305
EPOCH 0007 | Batch 600/765 | Loss: 0.016647
EPOCH 0007 | Batch 700/765 | Loss: 0.011527
===> EPOCH: 0007 | Train Loss: 0.0148 | Val PSNR: 18.49 | LR: 0.001000
EPOCH 0008 | Batch 100/765 | Loss: 0.016720
EPOCH 0008 | Batch 200/765 | Loss: 0.015336
EPOCH 0008 | Batch 300/765 | Loss: 0.018928
EPOCH 0008 | Batch 400/765 | Loss: 0.016843
EPOCH 0008 | Batch 500/765 | Loss: 0.018004
EPOCH 0008 | Batch 600/765 | Loss: 0.013837
EPOCH 0008 | Batch 700/765 | Loss: 0.018471
===> EPOCH: 0008 | Train Loss: 0.0149 | Val PSNR: 18.54 | LR: 0.001000
EPOCH 0009 | Batch 100/765 | Loss: 0.016161
EPOCH 0009 | Batch 200/765 | Loss: 0.013567
EPOCH 0009 | Batch 300/765 | Loss: 0.016323
EPOCH 0009 | Batch 400/765 | Loss: 0.011781
EPOCH 0009 | Batch 500/765 | Loss: 0.012648
EPOCH 0009 | Batch 600/765 | Loss: 0.013277
EPOCH 0009 | Batch 700/765 | Loss: 0.018402
Epoch    10: reducing learning rate of group 0 from 1.0000e-03 to 5.0000e-04.
===> EPOCH: 0009 | Train Loss: 0.0148 | Val PSNR: 18.48 | LR: 0.000500
EPOCH 0010 | Batch 100/765 | Loss: 0.014095
EPOCH 0010 | Batch 200/765 | Loss: 0.015660
EPOCH 0010 | Batch 300/765 | Loss: 0.016427
EPOCH 0010 | Batch 400/765 | Loss: 0.015780
EPOCH 0010 | Batch 500/765 | Loss: 0.012161
EPOCH 0010 | Batch 600/765 | Loss: 0.012687
EPOCH 0010 | Batch 700/765 | Loss: 0.016686
===> EPOCH: 0010 | Train Loss: 0.0148 | Val PSNR: 18.44 | LR: 0.000500
EPOCH 0011 | Batch 100/765 | Loss: 0.015504
EPOCH 0011 | Batch 200/765 | Loss: 0.015888
EPOCH 0011 | Batch 300/765 | Loss: 0.017547
EPOCH 0011 | Batch 400/765 | Loss: 0.014864
EPOCH 0011 | Batch 500/765 | Loss: 0.011758
EPOCH 0011 | Batch 600/765 | Loss: 0.017204
EPOCH 0011 | Batch 700/765 | Loss: 0.014739
===> EPOCH: 0011 | Train Loss: 0.0148 | Val PSNR: 18.57 | LR: 0.000500
[*] Best Model Updated! PSNR: 18.57
EPOCH 0012 | Batch 100/765 | Loss: 0.016792
EPOCH 0012 | Batch 200/765 | Loss: 0.015578
EPOCH 0012 | Batch 300/765 | Loss: 0.012510
EPOCH 0012 | Batch 400/765 | Loss: 0.011372
EPOCH 0012 | Batch 500/765 | Loss: 0.012563
EPOCH 0012 | Batch 600/765 | Loss: 0.011950
EPOCH 0012 | Batch 700/765 | Loss: 0.015277
===> EPOCH: 0012 | Train Loss: 0.0147 | Val PSNR: 18.56 | LR: 0.000500
EPOCH 0013 | Batch 100/765 | Loss: 0.014780
EPOCH 0013 | Batch 200/765 | Loss: 0.014597
EPOCH 0013 | Batch 300/765 | Loss: 0.015506
EPOCH 0013 | Batch 400/765 | Loss: 0.015329
EPOCH 0013 | Batch 500/765 | Loss: 0.015117
EPOCH 0013 | Batch 600/765 | Loss: 0.016211
EPOCH 0013 | Batch 700/765 | Loss: 0.014775
===> EPOCH: 0013 | Train Loss: 0.0148 | Val PSNR: 18.55 | LR: 0.000500
EPOCH 0014 | Batch 100/765 | Loss: 0.012491
EPOCH 0014 | Batch 200/765 | Loss: 0.016020
EPOCH 0014 | Batch 300/765 | Loss: 0.016084
EPOCH 0014 | Batch 400/765 | Loss: 0.013376
EPOCH 0014 | Batch 500/765 | Loss: 0.019430
EPOCH 0014 | Batch 600/765 | Loss: 0.014157
EPOCH 0014 | Batch 700/765 | Loss: 0.013986
===> EPOCH: 0014 | Train Loss: 0.0148 | Val PSNR: 18.57 | LR: 0.000500
[*] Best Model Updated! PSNR: 18.57
EPOCH 0015 | Batch 100/765 | Loss: 0.015480
EPOCH 0015 | Batch 200/765 | Loss: 0.012214
EPOCH 0015 | Batch 300/765 | Loss: 0.013178
EPOCH 0015 | Batch 400/765 | Loss: 0.016608
EPOCH 0015 | Batch 500/765 | Loss: 0.018361
EPOCH 0015 | Batch 600/765 | Loss: 0.015021
EPOCH 0015 | Batch 700/765 | Loss: 0.016724
===> EPOCH: 0015 | Train Loss: 0.0148 | Val PSNR: 18.58 | LR: 0.000500
[*] Best Model Updated! PSNR: 18.58
EPOCH 0016 | Batch 100/765 | Loss: 0.010945
EPOCH 0016 | Batch 200/765 | Loss: 0.011746
EPOCH 0016 | Batch 300/765 | Loss: 0.013062
EPOCH 0016 | Batch 400/765 | Loss: 0.017124
EPOCH 0016 | Batch 500/765 | Loss: 0.016321
EPOCH 0016 | Batch 600/765 | Loss: 0.018638
EPOCH 0016 | Batch 700/765 | Loss: 0.015361
===> EPOCH: 0016 | Train Loss: 0.0148 | Val PSNR: 18.54 | LR: 0.000500
EPOCH 0017 | Batch 100/765 | Loss: 0.013648
EPOCH 0017 | Batch 200/765 | Loss: 0.012851
EPOCH 0017 | Batch 300/765 | Loss: 0.014030
EPOCH 0017 | Batch 400/765 | Loss: 0.012816
EPOCH 0017 | Batch 500/765 | Loss: 0.013556
EPOCH 0017 | Batch 600/765 | Loss: 0.016143
EPOCH 0017 | Batch 700/765 | Loss: 0.017672
===> EPOCH: 0017 | Train Loss: 0.0148 | Val PSNR: 18.58 | LR: 0.000500
[*] Best Model Updated! PSNR: 18.58
EPOCH 0018 | Batch 100/765 | Loss: 0.014567
EPOCH 0018 | Batch 200/765 | Loss: 0.017563
EPOCH 0018 | Batch 300/765 | Loss: 0.013400
EPOCH 0018 | Batch 400/765 | Loss: 0.014566
EPOCH 0018 | Batch 500/765 | Loss: 0.015853
EPOCH 0018 | Batch 600/765 | Loss: 0.012655
EPOCH 0018 | Batch 700/765 | Loss: 0.014262
===> EPOCH: 0018 | Train Loss: 0.0148 | Val PSNR: 18.55 | LR: 0.000500
EPOCH 0019 | Batch 100/765 | Loss: 0.018332
EPOCH 0019 | Batch 200/765 | Loss: 0.013712
EPOCH 0019 | Batch 300/765 | Loss: 0.017333
EPOCH 0019 | Batch 400/765 | Loss: 0.019693
EPOCH 0019 | Batch 500/765 | Loss: 0.013773
EPOCH 0019 | Batch 600/765 | Loss: 0.015472
EPOCH 0019 | Batch 700/765 | Loss: 0.014392
===> EPOCH: 0019 | Train Loss: 0.0147 | Val PSNR: 18.58 | LR: 0.000500
[*] Best Model Updated! PSNR: 18.58