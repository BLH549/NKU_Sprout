Populating train/val lists...
Batches per Epoch: 767 | Batch Size: 32
Validation batches per Epoch: 85 | Batch Size: 32
device: cuda
EPOCH 0000 | Batch 100/767 | Loss: 0.027080
EPOCH 0000 | Batch 200/767 | Loss: 0.020741
EPOCH 0000 | Batch 300/767 | Loss: 0.017072
EPOCH 0000 | Batch 400/767 | Loss: 0.010644
EPOCH 0000 | Batch 500/767 | Loss: 0.013277
EPOCH 0000 | Batch 600/767 | Loss: 0.016414
EPOCH 0000 | Batch 700/767 | Loss: 0.013145
===> EPOCH: 0000 | Train Loss: 0.0230 | Val PSNR: 18.06 | LR: 0.001000
[*] Best Model Updated! PSNR: 18.06
EPOCH 0001 | Batch 100/767 | Loss: 0.016659
EPOCH 0001 | Batch 200/767 | Loss: 0.014579
EPOCH 0001 | Batch 300/767 | Loss: 0.015290
EPOCH 0001 | Batch 400/767 | Loss: 0.015452
EPOCH 0001 | Batch 500/767 | Loss: 0.018479
EPOCH 0001 | Batch 600/767 | Loss: 0.017087
EPOCH 0001 | Batch 700/767 | Loss: 0.012627
===> EPOCH: 0001 | Train Loss: 0.0152 | Val PSNR: 18.25 | LR: 0.001000
[*] Best Model Updated! PSNR: 18.25
EPOCH 0002 | Batch 100/767 | Loss: 0.012006
EPOCH 0002 | Batch 200/767 | Loss: 0.013593
EPOCH 0002 | Batch 300/767 | Loss: 0.011886
EPOCH 0002 | Batch 400/767 | Loss: 0.013696
EPOCH 0002 | Batch 500/767 | Loss: 0.012423
EPOCH 0002 | Batch 600/767 | Loss: 0.008506
EPOCH 0002 | Batch 700/767 | Loss: 0.012864
===> EPOCH: 0002 | Train Loss: 0.0149 | Val PSNR: 18.29 | LR: 0.001000
[*] Best Model Updated! PSNR: 18.29
EPOCH 0003 | Batch 100/767 | Loss: 0.014663
EPOCH 0003 | Batch 200/767 | Loss: 0.017565
EPOCH 0003 | Batch 300/767 | Loss: 0.012928
EPOCH 0003 | Batch 400/767 | Loss: 0.019517
EPOCH 0003 | Batch 500/767 | Loss: 0.013628
EPOCH 0003 | Batch 600/767 | Loss: 0.014721
EPOCH 0003 | Batch 700/767 | Loss: 0.012951
===> EPOCH: 0003 | Train Loss: 0.0149 | Val PSNR: 18.26 | LR: 0.001000
EPOCH 0004 | Batch 100/767 | Loss: 0.017616
EPOCH 0004 | Batch 200/767 | Loss: 0.016433
EPOCH 0004 | Batch 300/767 | Loss: 0.013444
EPOCH 0004 | Batch 400/767 | Loss: 0.014179
EPOCH 0004 | Batch 500/767 | Loss: 0.019581
EPOCH 0004 | Batch 600/767 | Loss: 0.017808
EPOCH 0004 | Batch 700/767 | Loss: 0.015164
===> EPOCH: 0004 | Train Loss: 0.0148 | Val PSNR: 18.28 | LR: 0.001000
EPOCH 0005 | Batch 100/767 | Loss: 0.014143
EPOCH 0005 | Batch 200/767 | Loss: 0.014621
EPOCH 0005 | Batch 300/767 | Loss: 0.013354
EPOCH 0005 | Batch 400/767 | Loss: 0.012906
EPOCH 0005 | Batch 500/767 | Loss: 0.012496
EPOCH 0005 | Batch 600/767 | Loss: 0.018204
EPOCH 0005 | Batch 700/767 | Loss: 0.013790
===> EPOCH: 0005 | Train Loss: 0.0148 | Val PSNR: 18.20 | LR: 0.000500
EPOCH 0006 | Batch 100/767 | Loss: 0.017483
EPOCH 0006 | Batch 200/767 | Loss: 0.012249
EPOCH 0006 | Batch 300/767 | Loss: 0.019705
EPOCH 0006 | Batch 400/767 | Loss: 0.014141
EPOCH 0006 | Batch 500/767 | Loss: 0.015154
EPOCH 0006 | Batch 600/767 | Loss: 0.012597
EPOCH 0006 | Batch 700/767 | Loss: 0.014447
===> EPOCH: 0006 | Train Loss: 0.0147 | Val PSNR: 18.29 | LR: 0.000500
EPOCH 0007 | Batch 100/767 | Loss: 0.015389
EPOCH 0007 | Batch 200/767 | Loss: 0.010798
EPOCH 0007 | Batch 300/767 | Loss: 0.016718
EPOCH 0007 | Batch 400/767 | Loss: 0.013435
EPOCH 0007 | Batch 500/767 | Loss: 0.016753
EPOCH 0007 | Batch 600/767 | Loss: 0.014391
EPOCH 0007 | Batch 700/767 | Loss: 0.015753
===> EPOCH: 0007 | Train Loss: 0.0147 | Val PSNR: 18.32 | LR: 0.000500
[*] Best Model Updated! PSNR: 18.32
EPOCH 0008 | Batch 100/767 | Loss: 0.012023
EPOCH 0008 | Batch 200/767 | Loss: 0.016461
EPOCH 0008 | Batch 300/767 | Loss: 0.014107
EPOCH 0008 | Batch 400/767 | Loss: 0.012762
EPOCH 0008 | Batch 500/767 | Loss: 0.016761
EPOCH 0008 | Batch 600/767 | Loss: 0.018627
EPOCH 0008 | Batch 700/767 | Loss: 0.012512
===> EPOCH: 0008 | Train Loss: 0.0147 | Val PSNR: 18.32 | LR: 0.000500
EPOCH 0009 | Batch 100/767 | Loss: 0.019443
EPOCH 0009 | Batch 200/767 | Loss: 0.018260
EPOCH 0009 | Batch 300/767 | Loss: 0.016528
EPOCH 0009 | Batch 400/767 | Loss: 0.017802
EPOCH 0009 | Batch 500/767 | Loss: 0.012918
EPOCH 0009 | Batch 600/767 | Loss: 0.015785
EPOCH 0009 | Batch 700/767 | Loss: 0.020024
===> EPOCH: 0009 | Train Loss: 0.0147 | Val PSNR: 18.32 | LR: 0.000500
EPOCH 0010 | Batch 100/767 | Loss: 0.014279
EPOCH 0010 | Batch 200/767 | Loss: 0.015576
EPOCH 0010 | Batch 300/767 | Loss: 0.018282
EPOCH 0010 | Batch 400/767 | Loss: 0.015681
EPOCH 0010 | Batch 500/767 | Loss: 0.020689
EPOCH 0010 | Batch 600/767 | Loss: 0.013276
EPOCH 0010 | Batch 700/767 | Loss: 0.015455
===> EPOCH: 0010 | Train Loss: 0.0147 | Val PSNR: 18.33 | LR: 0.000500
[*] Best Model Updated! PSNR: 18.33
EPOCH 0011 | Batch 100/767 | Loss: 0.012881
EPOCH 0011 | Batch 200/767 | Loss: 0.017074
EPOCH 0011 | Batch 300/767 | Loss: 0.014257
EPOCH 0011 | Batch 400/767 | Loss: 0.012834
EPOCH 0011 | Batch 500/767 | Loss: 0.015362
EPOCH 0011 | Batch 600/767 | Loss: 0.015895
EPOCH 0011 | Batch 700/767 | Loss: 0.012342
===> EPOCH: 0011 | Train Loss: 0.0146 | Val PSNR: 18.32 | LR: 0.000500
EPOCH 0012 | Batch 100/767 | Loss: 0.013316
EPOCH 0012 | Batch 200/767 | Loss: 0.017756
EPOCH 0012 | Batch 300/767 | Loss: 0.014481
EPOCH 0012 | Batch 400/767 | Loss: 0.015672
EPOCH 0012 | Batch 500/767 | Loss: 0.015367
EPOCH 0012 | Batch 600/767 | Loss: 0.020124
EPOCH 0012 | Batch 700/767 | Loss: 0.016041
===> EPOCH: 0012 | Train Loss: 0.0146 | Val PSNR: 18.31 | LR: 0.000500
EPOCH 0013 | Batch 100/767 | Loss: 0.014340
EPOCH 0013 | Batch 200/767 | Loss: 0.013277
EPOCH 0013 | Batch 300/767 | Loss: 0.017680
EPOCH 0013 | Batch 400/767 | Loss: 0.014111
EPOCH 0013 | Batch 500/767 | Loss: 0.012104
EPOCH 0013 | Batch 600/767 | Loss: 0.012642
EPOCH 0013 | Batch 700/767 | Loss: 0.012638
===> EPOCH: 0013 | Train Loss: 0.0146 | Val PSNR: 18.33 | LR: 0.000250
EPOCH 0014 | Batch 100/767 | Loss: 0.016023
EPOCH 0014 | Batch 200/767 | Loss: 0.017479
EPOCH 0014 | Batch 300/767 | Loss: 0.013588
EPOCH 0014 | Batch 400/767 | Loss: 0.016069
EPOCH 0014 | Batch 500/767 | Loss: 0.013722
EPOCH 0014 | Batch 600/767 | Loss: 0.014903
EPOCH 0014 | Batch 700/767 | Loss: 0.016460
===> EPOCH: 0014 | Train Loss: 0.0146 | Val PSNR: 18.32 | LR: 0.000250
EPOCH 0015 | Batch 100/767 | Loss: 0.013139
EPOCH 0015 | Batch 200/767 | Loss: 0.017521
EPOCH 0015 | Batch 300/767 | Loss: 0.014065
EPOCH 0015 | Batch 400/767 | Loss: 0.012957
EPOCH 0015 | Batch 500/767 | Loss: 0.015175
EPOCH 0015 | Batch 600/767 | Loss: 0.015629
EPOCH 0015 | Batch 700/767 | Loss: 0.012169
===> EPOCH: 0015 | Train Loss: 0.0146 | Val PSNR: 18.33 | LR: 0.000250
[*] Best Model Updated! PSNR: 18.33
EPOCH 0016 | Batch 100/767 | Loss: 0.015845
EPOCH 0016 | Batch 200/767 | Loss: 0.014126
EPOCH 0016 | Batch 300/767 | Loss: 0.015447
EPOCH 0016 | Batch 400/767 | Loss: 0.011016
EPOCH 0016 | Batch 500/767 | Loss: 0.015387
EPOCH 0016 | Batch 600/767 | Loss: 0.011413
EPOCH 0016 | Batch 700/767 | Loss: 0.013998
===> EPOCH: 0016 | Train Loss: 0.0146 | Val PSNR: 18.30 | LR: 0.000250
EPOCH 0017 | Batch 100/767 | Loss: 0.012675
EPOCH 0017 | Batch 200/767 | Loss: 0.011355
EPOCH 0017 | Batch 300/767 | Loss: 0.014063
EPOCH 0017 | Batch 400/767 | Loss: 0.014272
EPOCH 0017 | Batch 500/767 | Loss: 0.012989
EPOCH 0017 | Batch 600/767 | Loss: 0.013964
EPOCH 0017 | Batch 700/767 | Loss: 0.014390
===> EPOCH: 0017 | Train Loss: 0.0146 | Val PSNR: 18.31 | LR: 0.000250
EPOCH 0018 | Batch 100/767 | Loss: 0.015572
EPOCH 0018 | Batch 200/767 | Loss: 0.011768
EPOCH 0018 | Batch 300/767 | Loss: 0.016583
EPOCH 0018 | Batch 400/767 | Loss: 0.012033
EPOCH 0018 | Batch 500/767 | Loss: 0.014841
EPOCH 0018 | Batch 600/767 | Loss: 0.015624
EPOCH 0018 | Batch 700/767 | Loss: 0.015835
===> EPOCH: 0018 | Train Loss: 0.0146 | Val PSNR: 18.30 | LR: 0.000125
EPOCH 0019 | Batch 100/767 | Loss: 0.016262
EPOCH 0019 | Batch 200/767 | Loss: 0.016830
EPOCH 0019 | Batch 300/767 | Loss: 0.013613
EPOCH 0019 | Batch 400/767 | Loss: 0.011523
EPOCH 0019 | Batch 500/767 | Loss: 0.013568
EPOCH 0019 | Batch 600/767 | Loss: 0.015265
EPOCH 0019 | Batch 700/767 | Loss: 0.020442
===> EPOCH: 0019 | Train Loss: 0.0146 | Val PSNR: 18.34 | LR: 0.000125
[*] Best Model Updated! PSNR: 18.34